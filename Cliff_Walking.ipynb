{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "np.set_printoptions(threshold = np.inf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@params\n",
    "    array = state grid we constructed\n",
    "    row_number = #rows of state grid\n",
    "    column_number = #columns of state grid\n",
    "    episodes = #episodes\n",
    "    steps = #steps within episodes\n",
    "    alpha = learning rate\n",
    "    gama = discount factor\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def applyQlearning(array,row_number,column_number,episodes,steps,alpha,gama):\n",
    "    epsilon_q = 0.1\n",
    "    state_number = row_number * column_number\n",
    "    last_row_index = row_number - 1\n",
    "    last_column_index = column_number - 1\n",
    "    action_number = 4\n",
    "    q_table = np.random.rand(state_number, action_number)\n",
    "    # terminal state initialization\n",
    "    q_table[state_number - 1][0] = 0\n",
    "    q_table[state_number - 1][1] = 0\n",
    "    q_table[state_number - 1][2] = 0\n",
    "    q_table[state_number - 1][3] = 0\n",
    "    rewardArray = []\n",
    "    episodeArray = []\n",
    "    #   averageRewardArray = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    windowing_average_samples = 40\n",
    "    averageRewardArray = np.zeros(windowing_average_samples)\n",
    "    for i in range(episodes):  # loop for each episode\n",
    "        step = 0\n",
    "        #     if (i % 50 == 0 and epsilonNew > 0.01):\n",
    "         #   epsilonNew = epsilonNew - 0.01\n",
    "        current_state = array[3][0]  # initialize S\n",
    "        count = 0\n",
    "        totalReward = 0\n",
    "        episodeArray.append(i)\n",
    "        while (count in range(steps) and current_state.isTerminal(last_row_index) == 0):\n",
    "            step = step + 1\n",
    "            count = count + 1\n",
    "            # choose action A from S using policy derived from Q\n",
    "            action = current_state.eGreedy(q_table,epsilon_q, last_row_index,last_column_index)\n",
    "            # take action A, observe R,S'\n",
    "            next_state = current_state.takeAction(array, action, q_table,last_row_index,last_column_index,epsilon_q)\n",
    "            reward_got = next_state.getReward()\n",
    "            totalReward = totalReward + reward_got\n",
    "            actionMax = next_state.ePureGreedy(q_table,last_row_index,last_column_index)\n",
    "            q_table_max = q_table[(next_state.row * column_number) + next_state.column][actionMax]\n",
    "            q_table_current = q_table[(current_state.row * column_number) + current_state.column][action]\n",
    "            q_table[(current_state.row * column_number) + current_state.column][action] = q_table_current +\\\n",
    "                                            alpha * (reward_got + (gama * q_table_max) - q_table_current)  # update\n",
    "            current_state = next_state\n",
    "        averageRewardArray = shiftAndAddArray(averageRewardArray, totalReward)\n",
    "        valueToDraw = np.sum(averageRewardArray) / np.count_nonzero(averageRewardArray)\n",
    "        rewardArray.append(valueToDraw)\n",
    "    print(\"Q TRAINING IS OVER\")\n",
    "    episodeArray = episodeArray[10:]\n",
    "    rewardArray = rewardArray[10:]\n",
    "    drawPath(array, q_table,last_row_index,last_column_index,epsilon_q)\n",
    "    plt.plot(episodeArray, rewardArray)\n",
    "    plt.legend(['SARSA','Q learning'])\n",
    "    print(averageRewardArray)\n",
    "    plt.title(f'Smoothed by averaging {windowing_average_samples} successive episodes')\n",
    "    # limitting the overall boundary to have a nice vision\n",
    "    plt.ylim((-120, 0))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  this is for the test after the training\n",
    "def drawPath(array,Qvalue,last_row_index, last_column_index,epsilon):\n",
    "    current_state = array[last_row_index][0]  # initial state\n",
    "    greedyAction = current_state.ePureGreedy(Qvalue,last_row_index,last_column_index)  # action is determined\n",
    "    while(current_state.isTerminal(last_row_index) == 0):\n",
    "        next_state = current_state.takeAction(array,greedyAction,Qvalue,last_row_index, last_column_index,epsilon)\n",
    "        next_state.changeSign(\"X\")\n",
    "        followingAction = next_state.ePureGreedy(Qvalue,last_row_index, last_column_index)\n",
    "        current_state = next_state\n",
    "        greedyAction = followingAction\n",
    "    display2D(array)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "\n",
    "def display2D(array):\n",
    "    for row in array:\n",
    "        for column in row:\n",
    "            print(column.getSign(), end=\" \")\n",
    "        print(end=\"\\n\")\n",
    "\n",
    "def displayRewards(array):\n",
    "    for row in array:\n",
    "        for column in row:\n",
    "            print(column.getReward(), end=\" \")\n",
    "        print(end='\\n')\n",
    "\n",
    "def handleRewards(array, last_row, column):\n",
    "    for i in range(column - 2):\n",
    "        array[last_row][i + 1].setReward(-100)\n",
    "\n",
    "def shiftAndAddArray(array_to_shift, new_element):\n",
    "    for i in range(len(array_to_shift) - 1):\n",
    "        array_to_shift[i] = array_to_shift[i + 1]\n",
    "    array_to_shift[len(array_to_shift) - 1] = new_element\n",
    "    return array_to_shift\n",
    "\n",
    "class States:\n",
    "    def __init__(self, sign, reward, row, column):\n",
    "        self.sign = sign\n",
    "        self.reward = reward\n",
    "        self.row = row\n",
    "        self.column = column\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    def getSign(self):\n",
    "        return self.sign\n",
    "\n",
    "    def changeSign(self, new_sign):\n",
    "        self.sign = new_sign\n",
    "\n",
    "    def setReward(self, new_reward):\n",
    "        self.reward = new_reward\n",
    "\n",
    "    def eGreedy(self, q_value, epsilon, last_row_index, last_column_index):\n",
    "        new_action_list = [0, 1, 2, 3]  # 0 for right, 1 for down, 2 for up, 3 for left\n",
    "\n",
    "        if self.row == 0 and self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index and self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "            new_action_list.remove(1)\n",
    "        elif self.row == 0 and self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index and self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "            new_action_list.remove(1)\n",
    "        elif self.row == 0:\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index:\n",
    "            new_action_list.remove(1)\n",
    "        elif self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "        elif self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "\n",
    "        probability = random.uniform(0, 1)\n",
    "\n",
    "        if probability < 1 - epsilon:\n",
    "            return self.findMaxIndex(q_value, new_action_list, last_row_index, last_column_index)\n",
    "        else:\n",
    "            return random.choice(new_action_list)\n",
    "\n",
    "    def ePureGreedy(self, q_value, last_row_index, last_column_index):\n",
    "        new_action_list = [0, 1, 2, 3]\n",
    "\n",
    "        if self.row == 0 and self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index and self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "            new_action_list.remove(1)\n",
    "        elif self.row == 0 and self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index and self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "            new_action_list.remove(1)\n",
    "        elif self.row == 0:\n",
    "            new_action_list.remove(2)\n",
    "        elif self.row == last_row_index:\n",
    "            new_action_list.remove(1)\n",
    "        elif self.column == 0:\n",
    "            new_action_list.remove(3)\n",
    "        elif self.column == last_column_index:\n",
    "            new_action_list.remove(0)\n",
    "\n",
    "        return self.findMaxIndex(q_value, new_action_list, last_row_index, last_column_index)\n",
    "\n",
    "    def takeAction(self, array, action, q_value, last_row_index, last_column_index, epsilon):\n",
    "        if self.row == 0 and action == 2:\n",
    "            new_action = self.eGreedy(q_value, epsilon, last_row_index, last_column_index)\n",
    "            return self.takeAction(array, new_action, q_value, last_row_index, last_column_index, epsilon)\n",
    "        elif self.row == last_row_index and action == 1:\n",
    "            new_action = self.eGreedy(q_value, epsilon, last_row_index, last_column_index)\n",
    "            return self.takeAction(array, new_action, q_value, last_row_index, last_column_index, epsilon)\n",
    "        elif self.column == last_column_index and action == 0:\n",
    "            new_action = self.eGreedy(q_value, epsilon, last_row_index, last_column_index)\n",
    "            return self.takeAction(array, new_action, q_value, last_row_index, last_column_index, epsilon)\n",
    "        elif self.column == 0 and action == 3:\n",
    "            new_action = self.eGreedy(q_value, epsilon, last_row_index, last_column_index)\n",
    "            return self.takeAction(array, new_action, q_value, last_row_index, last_column_index, epsilon)\n",
    "        else:\n",
    "            if action == 0:\n",
    "                next_state = array[self.row][self.column + 1]\n",
    "            elif action == 1:\n",
    "                next_state = array[self.row + 1][self.column]\n",
    "            elif action == 2:\n",
    "                next_state = array[self.row - 1][self.column]\n",
    "            elif action == 3:\n",
    "                next_state = array[self.row][self.column - 1]\n",
    "        return next_state\n",
    "\n",
    "    def findMaxIndex(self, q_value, action_list, last_row_index, last_column_index):\n",
    "        max_index = np.argmax(q_value[self.row * 12 + self.column])\n",
    "        return max_index\n",
    "\n",
    "    def isTerminal(self, last_row_index):\n",
    "        if self.row == last_row_index and self.column > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def applyQlearning(array, row_number, column_number, episodes, steps, alpha, gamma, render=False):\n",
    "    epsilon_q = 0.1\n",
    "    state_number = row_number * column_number\n",
    "    last_row_index = row_number - 1\n",
    "    last_column_index = column_number - 1\n",
    "    action_number = 4\n",
    "    q_table = np.zeros((state_number, action_number))\n",
    "\n",
    "    reward_array = []\n",
    "    episode_array = []\n",
    "    windowing_average_samples = 40\n",
    "    average_reward_array = np.zeros(windowing_average_samples)\n",
    "\n",
    "    env = gym.make('CliffWalking-v0')  # Create the CliffWalking environment\n",
    "\n",
    "    for i in range(episodes):\n",
    "        step = 0\n",
    "        current_state = States('S', 0, 3, 0)  # Assuming starting state, adjust accordingly\n",
    "        count = 0\n",
    "        total_reward = 0\n",
    "        episode_array.append(i)\n",
    "\n",
    "        while count in range(steps) and not current_state.isTerminal(last_row_index):\n",
    "            step += 1\n",
    "            count += 1\n",
    "\n",
    "            action = current_state.eGreedy(q_table, epsilon_q, last_row_index, last_column_index)\n",
    "            next_state_info = current_state.takeAction(array, action, q_table, last_row_index, last_column_index, epsilon_q)\n",
    "            next_state = States(*next_state_info)\n",
    "\n",
    "            reward_got = next_state.getReward()\n",
    "            total_reward += reward_got\n",
    "            action_max = next_state.ePureGreedy(q_table, last_row_index, last_column_index)\n",
    "            q_table_max = np.max(q_table[(next_state.row * column_number) + next_state.column, :])\n",
    "            q_table_current = q_table[(current_state.row * column_number) + current_state.column, action]\n",
    "\n",
    "            q_table[(current_state.row * column_number) + current_state.column, action] = (\n",
    "                q_table_current + alpha * (reward_got + gamma * q_table_max - q_table_current)\n",
    "            )\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        average_reward_array = shiftAndAddArray(average_reward_array, total_reward)\n",
    "        value_to_draw = np.sum(average_reward_array) / np.count_nonzero(average_reward_array)\n",
    "        reward_array.append(value_to_draw)\n",
    "\n",
    "        # Display and handle rewards\n",
    "        displayRewards(array)\n",
    "        handleRewards(array, last_row_index, column_number)\n",
    "\n",
    "    print(\"Q TRAINING IS OVER\")\n",
    "    episode_array = episode_array[10:]\n",
    "    reward_array = reward_array[10:]\n",
    "\n",
    "    plt.plot(episode_array, reward_array)\n",
    "    plt.legend(['SARSA', 'Q learning'])\n",
    "    plt.title(f'Smoothed by averaging {windowing_average_samples} successive episodes')\n",
    "    plt.ylim((-120, 0))\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Example usage with your state grid\n",
    "# Replace the following lines with your specific state grid, row_number, column_number, etc.\n",
    "array = [[States('O', -1, i, j) for j in range(4)] for i in range(4)]\n",
    "row_number = 4\n",
    "column_number = 4\n",
    "episodes = 500\n",
    "steps = 100\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "applyQlearning(array, row_number, column_number, episodes, steps, alpha, gamma, render=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Your States class implementation here\n",
    "\n",
    "def display2D(array):\n",
    "    for row in array:\n",
    "        for column in row:\n",
    "            print(column.getSign(), end=\" \")\n",
    "        print(end=\"\\n\")\n",
    "\n",
    "def displayRewards(array):\n",
    "    for row in array:\n",
    "        for column in row:\n",
    "            print(column.getReward(), end=\" \")\n",
    "        print(end='\\n')\n",
    "\n",
    "def handleRewards(array, last_row, column):\n",
    "    for i in range(column - 2):\n",
    "        array[last_row][i + 1].setReward(-100)\n",
    "\n",
    "def shiftAndAddArray(array_to_shift, new_element):\n",
    "    for i in range(len(array_to_shift) - 1):\n",
    "        array_to_shift[i] = array_to_shift[i + 1]\n",
    "    array_to_shift[len(array_to_shift) - 1] = new_element\n",
    "    return array_to_shift\n",
    "\n",
    "def applyQlearning(array, row_number, column_number, episodes, steps, alpha, gamma, render=False):\n",
    "    epsilon_q = 0.1\n",
    "    state_number = row_number * column_number\n",
    "    last_row_index = row_number - 1\n",
    "    last_column_index = column_number - 1\n",
    "    action_number = 4\n",
    "    q_table = np.zeros((state_number, action_number))\n",
    "\n",
    "    reward_array = []\n",
    "    episode_array = []\n",
    "    windowing_average_samples = 40\n",
    "    average_reward_array = np.zeros(windowing_average_samples)\n",
    "\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "\n",
    "    for i in range(episodes):\n",
    "        step = 0\n",
    "        current_state = States('S', 0, 3, 0)\n",
    "        count = 0\n",
    "        total_reward = 0\n",
    "        episode_array.append(i)\n",
    "\n",
    "        while count in range(steps) and not current_state.isTerminal(last_row_index):\n",
    "            step += 1\n",
    "            count += 1\n",
    "\n",
    "            action = current_state.eGreedy(q_table, epsilon_q, last_row_index, last_column_index)\n",
    "            next_state_info = current_state.takeAction(array, action, q_table, last_row_index, last_column_index, epsilon_q)\n",
    "            next_state = States(*next_state_info)\n",
    "\n",
    "            reward_got = next_state.getReward()\n",
    "            total_reward += reward_got\n",
    "            action_max = next_state.ePureGreedy(q_table, last_row_index, last_column_index)\n",
    "            q_table_max = np.max(q_table[(next_state.row * column_number) + next_state.column, :])\n",
    "            q_table_current = q_table[(current_state.row * column_number) + current_state.column, action]\n",
    "\n",
    "            q_table[(current_state.row * column_number) + current_state.column, action] = (\n",
    "                q_table_current + alpha * (reward_got + gamma * q_table_max - q_table_current)\n",
    "            )\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        average_reward_array = shiftAndAddArray(average_reward_array, total_reward)\n",
    "        value_to_draw = np.sum(average_reward_array) / np.count_nonzero(average_reward_array)\n",
    "        reward_array.append(value_to_draw)\n",
    "\n",
    "        # Display and handle rewards\n",
    "        displayRewards(array)\n",
    "        handleRewards(array, last_row_index, column_number)\n",
    "\n",
    "    print(\"Q TRAINING IS OVER\")\n",
    "    episode_array = episode_array[10:]\n",
    "    reward_array = reward_array[10:]\n",
    "\n",
    "    plt.plot(episode_array, reward_array)\n",
    "    plt.legend(['SARSA', 'Q learning'])\n",
    "    plt.title(f'Smoothed by averaging {windowing_average_samples} successive episodes')\n",
    "    plt.ylim((-120, 0))\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def applySarsa(array, row_number, column_number, episodes, steps, alpha, gamma, render=False):\n",
    "    epsilon_q = 0.1\n",
    "    state_number = row_number * column_number\n",
    "    last_row_index = row_number - 1\n",
    "    last_column_index = column_number - 1\n",
    "    action_number = 4\n",
    "    q_table = np.zeros((state_number, action_number))\n",
    "\n",
    "    reward_array = []\n",
    "    episode_array = []\n",
    "    windowing_average_samples = 40\n",
    "    average_reward_array = np.zeros(windowing_average_samples)\n",
    "\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "\n",
    "    for i in range(episodes):\n",
    "        step = 0\n",
    "        current_state = States('S', 0, 3, 0)\n",
    "        count = 0\n",
    "        total_reward = 0\n",
    "        episode_array.append(i)\n",
    "\n",
    "        while count in range(steps) and not current_state.isTerminal(last_row_index):\n",
    "            step += 1\n",
    "            count += 1\n",
    "\n",
    "            action = current_state.eGreedy(q_table, epsilon_q, last_row_index, last_column_index)\n",
    "            next_state_info = current_state.takeAction(array, action, q_table, last_row_index, last_column_index, epsilon_q)\n",
    "            next_state = States(*next_state_info)\n",
    "\n",
    "            reward_got = next_state.getReward()\n",
    "            total_reward += reward_got\n",
    "            next_action = next_state.eGreedy(q_table, epsilon_q, last_row_index, last_column_index)\n",
    "            q_table_max = np.max(q_table[(next_state.row * column_number) + next_state.column, :])\n",
    "            q_table_current = q_table[(current_state.row * column_number) + current_state.column, action]\n",
    "\n",
    "            q_table[(current_state.row * column_number) + current_state.column, action] = (\n",
    "                q_table_current + alpha * (reward_got + gamma * q_table_max - q_table_current)\n",
    "            )\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        average_reward_array = shiftAndAddArray(average_reward_array, total_reward)\n",
    "        value_to_draw = np.sum(average_reward_array) / np.count_nonzero(average_reward_array)\n",
    "        reward_array.append(value_to_draw)\n",
    "\n",
    "        # Display and handle rewards\n",
    "        displayRewards(array)\n",
    "        handleRewards(array, last_row_index, column_number)\n",
    "\n",
    "    print(\"SARSA TRAINING IS OVER\")\n",
    "    episode_array = episode_array[10:]\n",
    "    reward_array = reward_array[10:]\n",
    "\n",
    "    plt.plot(episode_array, reward_array)\n",
    "    plt.legend(['SARSA', 'Q learning'])\n",
    "    plt.title(f'Smoothed by averaging {windowing_average_samples} successive episodes')\n",
    "    plt.ylim((-120, 0))\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def main():\n",
    "    episode_number = 600\n",
    "    steps = 500\n",
    "    gamma = 0.9\n",
    "    alpha = 0.25\n",
    "    row_number = 4\n",
    "    column_number = 12\n",
    "\n",
    "    stateGrid = [[States('O',-1,y,x) for x in range (column_number)] for y in range(row_number)]\n",
    "\n",
    "    display2D(stateGrid)\n",
    "    print(\"OK HERE WE STOP\")\n",
    "    stateGrid[row_number - 1][0].changeSign('S')\n",
    "    stateGrid[row_number - 1][column_number - 1].changeSign('F')\n",
    "    for i in range(10):\n",
    "        stateGrid[row_number - 1][i+1].changeSign('C')\n",
    "\n",
    "    display2D(stateGrid)\n",
    "    handleRewards(stateGrid, row_number - 1, column_number)\n",
    "    displayRewards(stateGrid)\n",
    "    print(\"CONSTRUCTED\")\n",
    "\n",
    "    stateGrid2 = copy.deepcopy(stateGrid)\n",
    "    applySarsa(stateGrid, row_number, column_number, episode_number, steps, alpha, gamma)\n",
    "    applyQlearning(stateGrid2, row_number, column_number, episode_number, steps, alpha, gamma)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
